{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "legitimate-baltimore",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = 'C:/Users/ADMIN11/Desktop/GUDDU TEFL/pdf/'\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "automatic-piano",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "independent-spotlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = os.listdir()   #this variable will have a list of all the files from the path provided "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "colored-shoot",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['COVER LETTER.pdf', 'Resume-Ariana Gokak.pdf']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "coastal-stream",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Email:\n",
      "\n",
      "pranitathorat23@gmail.com\n",
      "\n",
      "Phone:\n",
      "\n",
      "8605492445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cont = []\n",
    "for i in file_list:\n",
    "    f = open(path+i,'rb')     # opens the respective file in read mode\n",
    "    reader = PyPDF2.PdfFileReader(f) # using PdfReader function from PyPDF2 liabrary to read the page\n",
    "    contents = reader.getPage(0).extractText().split(' ') \n",
    "    # select the first page extract all the information the page \n",
    "    # further we will split the information on the bases of the seperator as per need \n",
    "    cont.append(contents)\n",
    "    details = ''\n",
    "    # further we will apply a for loop and try to extract what info we need\n",
    "    for i in range(len(contents)):\n",
    "        if contents[i].find('Email')!=-1 or contents[i].find('Phone')!=-1:\n",
    "            details+=contents[i]+contents[i+1]\n",
    "print(details)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-slave",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "infrared-halloween",
   "metadata": {},
   "source": [
    "same ways we can extract information from word pages or we can scrape web pages to collect data on which further we apply nlp techniques. Lets jump to nlp as of now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "romance-starter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "emotional-grill",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sexual-light",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "synthetic-typing",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_string = '''Seeking a challenging position in a technical organization. Seek opportunities and improve technical skills.\n",
    "Advance with technology in conjunction with the organizational goals.Cohort Analysis: \n",
    "Building future strategies and analyzing retention rate per cohort using Cohort Analysis.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "frank-commodity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "coupled-briefing",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_string = raw_string.casefold() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "consolidated-portal",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = lower_string.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fundamental-level",
   "metadata": {},
   "source": [
    "First thing that we ll see is tokenisation: It means converting a given input to a respective word or a sentence like making it have its own individual identity in the form of a list element or a string element but seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "expanded-being",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_docs = [\"I am writing some very basic english sentences\",\n",
    "\"I'm just writing it for the demo PURPOSE to make audience understand the basics .\",\n",
    "\"The point is to _learn HOW it works_ on #simple # data.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guilty-notification",
   "metadata": {},
   "source": [
    "# Converting to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "western-caribbean",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i am writing some very basic english sentences', \"i'm just writing it for the demo purpose to make audience understand the basics .\", 'the point is to _learn how it works_ on #simple # data.']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "raw_docs = [doc.lower() for doc in raw_docs]\n",
    "print(raw_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-ambassador",
   "metadata": {},
   "source": [
    "# Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supported-direction",
   "metadata": {},
   "source": [
    "* what tokenisation means?\n",
    "1. break the sentence in tokens ie individual words\n",
    "2. understand the importance of each word with respect to the sentence\n",
    "3. Derive meaningful insights from the cluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "identified-imagination",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "unlikely-armor",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_doc = [word_tokenize(doc) for doc in raw_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "designed-yugoslavia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i', 'am', 'writing', 'some', 'very', 'basic', 'english', 'sentences'],\n",
       " ['i',\n",
       "  \"'m\",\n",
       "  'just',\n",
       "  'writing',\n",
       "  'it',\n",
       "  'for',\n",
       "  'the',\n",
       "  'demo',\n",
       "  'purpose',\n",
       "  'to',\n",
       "  'make',\n",
       "  'audience',\n",
       "  'understand',\n",
       "  'the',\n",
       "  'basics',\n",
       "  '.'],\n",
       " ['the',\n",
       "  'point',\n",
       "  'is',\n",
       "  'to',\n",
       "  '_learn',\n",
       "  'how',\n",
       "  'it',\n",
       "  'works_',\n",
       "  'on',\n",
       "  '#',\n",
       "  'simple',\n",
       "  '#',\n",
       "  'data',\n",
       "  '.']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "excited-transfer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "primary-guarantee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_sent = [sent_tokenize(doc) for doc in raw_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "modern-facility",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i am writing some very basic english sentences'],\n",
       " [\"i'm just writing it for the demo purpose to make audience understand the basics .\"],\n",
       " ['the point is to _learn how it works_ on #simple # data.']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_sent[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-regulation",
   "metadata": {},
   "source": [
    "# Removing punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-screen",
   "metadata": {},
   "source": [
    "punctuations refer to those special characters which come along with every sentence like . _ - # and so on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "electoral-person",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "increasing-hacker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'am', 'writing', 'some', 'very', 'basic', 'english', 'sentences'], ['i', 'm', 'just', 'writing', 'it', 'for', 'the', 'demo', 'purpose', 'to', 'make', 'audience', 'understand', 'the', 'basics'], ['the', 'point', 'is', 'to', 'learn', 'how', 'it', 'works', 'on', 'simple', 'data']]\n"
     ]
    }
   ],
   "source": [
    "tokenized_docs_no_punctuation = []\n",
    "\n",
    "for review in tokenize_doc:\n",
    "    new_review = []\n",
    "    for token in review:\n",
    "        new_token = regex.sub(u'', token)\n",
    "        if not new_token == u'':\n",
    "            new_review.append(new_token)\n",
    "    \n",
    "    tokenized_docs_no_punctuation.append(new_review)\n",
    "    \n",
    "print(tokenized_docs_no_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alone-armenia",
   "metadata": {},
   "source": [
    "Use of regex in punctuation is not well understood: please refer it with someone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-condition",
   "metadata": {},
   "source": [
    "# Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "nearby-friendly",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "norman-slave",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['writing', 'basic', 'english', 'sentences'], ['writing', 'demo', 'purpose', 'make', 'audience', 'understand', 'basics'], ['point', 'learn', 'works', 'simple', 'data']]\n"
     ]
    }
   ],
   "source": [
    "tokenize_doc_no_stopwords = []\n",
    "\n",
    "for i in tokenized_docs_no_punctuation:\n",
    "    new_vector_array = []\n",
    "    for j in i:\n",
    "        if not j in stopwords.words('English'):\n",
    "            new_vector_array.append(j)\n",
    "            \n",
    "    tokenize_doc_no_stopwords.append(new_vector_array)\n",
    "print(tokenize_doc_no_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "animal-surrey",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['writing', 'basic', 'english', 'sentences'],\n",
       " ['writing', 'demo', 'purpose', 'make', 'audience', 'understand', 'basics'],\n",
       " ['point', 'learn', 'works', 'simple', 'data']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdnc = []\n",
    "\n",
    "for i in tokenized_docs_no_punctuation:\n",
    "    new_vec = []\n",
    "    for j in i:\n",
    "        if j not in stopwords.words('English'):\n",
    "            new_vec.append(j)\n",
    "    tdnc.append(new_vec)\n",
    "tdnc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respiratory-updating",
   "metadata": {},
   "source": [
    "# Stemming and Lemmantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "proud-exhaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "constant-identity",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "wordnet = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-ultimate",
   "metadata": {},
   "source": [
    "First we will try out stemming where we try to extract the base word (ie in playing, played, playable the word play is basic.\n",
    "Drawbacks: this technique does not provide the correct word everytime and can lead to meaningless words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "valued-serial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['write', 'basic', 'english', 'sentenc'], ['write', 'demo', 'purpos', 'make', 'audienc', 'understand', 'basic'], ['point', 'learn', 'work', 'simpl', 'data']]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_doc_stemmed = []\n",
    "\n",
    "for i in tokenize_doc_no_stopwords:\n",
    "    final_doc = []\n",
    "    for j in i:\n",
    "        final_doc.append(porter.stem(j))\n",
    "    preprocessed_doc_stemmed.append(final_doc)\n",
    "\n",
    "print(preprocessed_doc_stemmed)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-librarian",
   "metadata": {},
   "source": [
    "lemmantisation is an upgraded version of stemming where words are restored to their basic form. lemmatisation is more in use as compared to stemming. In general the output of lemmatisation is more specific and correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "together-relief",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['writing', 'basic', 'english', 'sentence'], ['writing', 'demo', 'purpose', 'make', 'audience', 'understand', 'basic'], ['point', 'learn', 'work', 'simple', 'data']]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_doc_lemmatize = []\n",
    "\n",
    "for i in tokenize_doc_no_stopwords:\n",
    "    final_doc_lemm = []\n",
    "    for j in i:\n",
    "        final_doc_lemm.append(wordnet.lemmatize(j))\n",
    "    preprocessed_doc_lemmatize.append(final_doc_lemm)\n",
    "\n",
    "print(preprocessed_doc_lemmatize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specialized-jumping",
   "metadata": {},
   "source": [
    "# Hotel review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "surprising-january",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import nltk.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daily-portrait",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abc', 'abc.zip', 'alpino', 'alpino.zip', 'biocreative_ppi', 'biocreative_ppi.zip', 'brown', 'brown.zip', 'brown_tei', 'brown_tei.zip', 'cess_cat', 'cess_cat.zip', 'cess_esp', 'cess_esp.zip', 'chat80', 'chat80.zip', 'city_database', 'city_database.zip', 'cmudict', 'cmudict.zip', 'comparative_sentences', 'comparative_sentences.zip', 'comtrans.zip', 'conll2000', 'conll2000.zip', 'conll2002', 'conll2002.zip', 'conll2007.zip', 'crubadan', 'crubadan.zip', 'dependency_treebank', 'dependency_treebank.zip', 'dolch', 'dolch.zip', 'europarl_raw', 'europarl_raw.zip', 'floresta', 'floresta.zip', 'framenet_v15', 'framenet_v15.zip', 'framenet_v17', 'framenet_v17.zip', 'gazetteers', 'gazetteers.zip', 'genesis', 'genesis.zip', 'gutenberg', 'gutenberg.zip', 'ieer', 'ieer.zip', 'inaugural', 'inaugural.zip', 'indian', 'indian.zip', 'jeita.zip', 'kimmo', 'kimmo.zip', 'knbc.zip', 'lin_thesaurus', 'lin_thesaurus.zip', 'machado.zip', 'mac_morpho', 'mac_morpho.zip', 'masc_tagged.zip', 'movie_reviews', 'movie_reviews.zip', 'mte_teip5', 'mte_teip5.zip', 'names', 'names.zip', 'nombank.1.0.zip', 'nonbreaking_prefixes', 'nonbreaking_prefixes.zip', 'nps_chat', 'nps_chat.zip', 'omw', 'omw.zip', 'opinion_lexicon', 'opinion_lexicon.zip', 'panlex_swadesh.zip', 'paradigms', 'paradigms.zip', 'pil', 'pil.zip', 'pl196x', 'pl196x.zip', 'ppattach', 'ppattach.zip', 'problem_reports', 'problem_reports.zip', 'product_reviews_1', 'product_reviews_1.zip', 'product_reviews_2', 'product_reviews_2.zip', 'propbank.zip', 'pros_cons', 'pros_cons.zip', 'ptb', 'ptb.zip', 'qc', 'qc.zip', 'reuters.zip', 'rte', 'rte.zip', 'semcor.zip', 'senseval', 'senseval.zip', 'sentence_polarity', 'sentence_polarity.zip', 'sentiwordnet', 'sentiwordnet.zip', 'shakespeare', 'shakespeare.zip', 'sinica_treebank', 'sinica_treebank.zip', 'smultron', 'smultron.zip', 'state_union', 'state_union.zip', 'stopwords', 'stopwords.zip', 'subjectivity', 'subjectivity.zip', 'swadesh', 'swadesh.zip', 'switchboard', 'switchboard.zip', 'timit', 'timit.zip', 'toolbox', 'toolbox.zip', 'treebank', 'treebank.zip', 'twitter_samples', 'twitter_samples.zip', 'udhr', 'udhr.zip', 'udhr2', 'udhr2.zip', 'unicode_samples', 'unicode_samples.zip', 'universal_treebanks_v20.zip', 'verbnet', 'verbnet.zip', 'verbnet3', 'verbnet3.zip', 'webtext', 'webtext.zip', 'wordnet', 'wordnet.zip', 'wordnet31', 'wordnet31.zip', 'wordnet_ic', 'wordnet_ic.zip', 'words', 'words.zip', 'ycoe', 'ycoe.zip']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(nltk.data.find('corpora')))\n",
    "# list of different files and functionality in nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "leading-russell",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "brown.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "brazilian-drinking",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "derived-envelope",
   "metadata": {},
   "outputs": [],
   "source": [
    "hamlet = nltk.corpus.gutenberg.words('shakespeare-hamlet.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "hungarian-orleans",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37360"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hamlet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "deluxe-cream",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.corpus.reader.util.StreamBackedCorpusView"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(hamlet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
